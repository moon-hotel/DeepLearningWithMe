## 代码索引

- [This post is all you need（①多头注意力机制原理）](https://www.ylkz.life/deeplearning/p10553832/) 
- [This post is all you need（②位置编码与编码解码过程）](https://www.ylkz.life/deeplearning/p10770524/)
- [This post is all you need（③网络结构与自注意力实现）](https://www.ylkz.life/deeplearning/p12158901/)
  - [自注意力机制实现 代码](01_TransformerTranslation/README.md)
- [This post is all you need（④Transformer的实现过程）](https://www.ylkz.life/deeplearning/p10391698/)
  - [Transformer实现 代码](01_TransformerTranslation/README.md)
- [This post is all you need（⑤基于Transformer的翻译模型）](https://www.ylkz.life/deeplearning/p10667939/)
  - [翻译模型 代码](01_TransformerTranslation/README.md)
- [This post is all you need（⑥基于Transformer的分类模型）](https://www.ylkz.life/deeplearning/p10550146/)
  - [文本分类模型 代码](02_TransformerClassification/README.md)
- [This post is all you need（⑦基于Transformer的对联模型）](https://www.ylkz.life/deeplearning/p11017569/)
  - [对联生成模型 代码](03_TransformerCouplet/README.md)
- [This post is all you need（层层剥开Transformer）](https://mp.weixin.qq.com/s/uch_AGcSB8OSAeVu2sme8A)
- [BERT原理与NSP和MLM](https://www.ylkz.life/deeplearning/p10631450/)
- [从零实现BERT网络模型](https://mp.weixin.qq.com/s/8X9yr0n0xKt8dsh1ZoEr_A)
  - [BERT实现 代码](https://github.com/moon-hotel/BertWithPretrained)
- [基于BERT预训练模型的中文文本分类任务 代码](https://github.com/moon-hotel/BertWithPretrained)
- [基于BERT预训练模型的英文文本蕴含任务 代码](https://github.com/moon-hotel/BertWithPretrained)
- [基于BERT预训练模型的英文多选项(SWAG)任务](https://github.com/moon-hotel/BertWithPretrained)
- [基于BERT预训练模型的英文问答(SQuAD)任务](https://github.com/moon-hotel/BertWithPretrained) 
- [基于NSL和MLM任务从头训练BERT任务](https://github.com/moon-hotel/BertWithPretrained)
### [返回主页](../README.md)