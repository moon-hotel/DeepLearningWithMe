

[TOC]



# 3.1 线性回归

经过前面预备知识的介绍，现在终于正式进入到了深度学习的内容介绍中。那什么又是深度学习呢？以及我们为什么需要深度学习呢？要想弄清楚这两个问题，我们还得先从机器学习中的线性回归说起。

## 3.1.1 房价预测

通常来讲，我们所学的每个算法都是为了解决某一类问题而诞生的。换句话说，也就是在实际情况中的确存在一些问题能够通过线性回归来解决，例如对房价的预测，但是有人可能会问，为什么对于房价的预测就应该用线性回归，而不是其他算法模型呢？其原因就在于常识告诉我们房价是随着面积的增长而增长的，且总体上呈线性增长的趋势。那有没有当面积大到一定程度后价格反而降低，因此不符合线性增长的呢？这当然也可能存在，但在实际处理中肯定会优先选择线性回归模型，当效果不佳时我们会尝试其他算法，因此，当学习过多个算法模型后，在得到某个具体的问题时，可能就需要考虑哪种模型更适合来解决这个问题了。

### 3.1.1.1 理解线性回归模型

例如某市的房价走势如图3-1所示，其中横坐标为面积，纵坐标为价格，并且房价整体上呈线性增长的趋势。假如现在随意告诉你一个房屋的面积，要怎样才能预测（或者叫计算）出其对应的价格呢？

<div align=center>
<img width="400" src="../img/p3_1.png"/>
</div>

<center>
  图 3-1 某市的房价走势图
</center>

### 3.1.1.2 建立线性回归模型

一般来讲，当我们得到一个实际问题时，首先会根据问题的背景结合常识选择一个合适的模型。同时，现在常识告诉我们房价的增长更优先符合线性回归这类模型，因此可以考虑建立一个如下所示的线性回归模型（Linear Regression）。

$$
\hat{y}=h(x)=wx+b\tag{3-1}
$$
其中$w$叫权重（Weight）或模型参数（Parameter），$b$叫偏置（Bias）或者截距（Intercept）。当通过某种方法求解得到未知参数$w$和$b$之后，也就意味着我们得到了这个预测模型，即给定一个房屋面积$x$，就能够预测出其对应的房价$\hat{y}$。

注意： 在机器学习中所谓的模型，可以简单理解为一个函数。

当然，尽管影响房价的主要因素是面积，但是其他因素同样也可能影响房屋的价格。例如房屋到学校的距离、到医院的距离和到大型商场的距离等，只是各个维度对应的权重大小不同而已。虽然现实生活中一般不这么量化，但是开发商也总是会拿学区房做卖点，所以这时便有了影响房价的4个因素，而在机器学习中我们将其称为特征（Feature）或者属性（Attribute），因此，包含多个特征的线性回归就叫作多变量线性回归（Multiple Linear Regression）。

此时，便可以得到如下所示的线性回归模型。
$$
\hat{y}=h(x)={{w}_{1}}{{x}_{1}}+\cdots +{{w}_{4}}{{x}_{4}}+b=w^Tx+b\tag{3-2}
$$
其中$x_1,x_2,x_3,x_4$表示输入的4项房屋信息特征，$w_1,w_2,w_3,w_4$表示每个特征对应的权重参数，$b$ 为偏置。

并且我们还可以通过如下示意图来对式(3-2)中的模型进行表示：

<img src="https://moonhotel.oss-cn-shanghai.aliyuncs.com/images/000102.png" style="zoom:70%;" />

<center>
    图 3-2. 房价预测线性回归结构图（偏置未画出）
</center>

### 3.1.1.3 求解线性回归模型

当建立好一个模型后，自然而然想到的就是如何通过给定的数据，也叫训练集（Training Data），来对模型 $h(x)$进行求解。在中学时期我们学过如何通过两个坐标点来求解过这两点的直线，可在上述的场景中这种做法显然是行不通的（因为求解线性回归模型所有的点并不在一条直线上），那有没有什么好的解决的办法呢？

此时就需要我们转换一下思路了，既然不能直接进行求解，那就换一种间接的方式。现在来想象一下，当 $h(x)$ 满足一个什么样的条件时，它才能称得上是一个好的$h(x)$？ 回想一下求解$h(x)$的目的是什么，不就是希望输入面积$x$后能够输出“准确”的房价$\hat{y}$吗？既然直接求解$h(x)$不好入手，那么我们就从“准确”来入手。

可又怎样来定义准确呢？在这里，我们可以通过计算每个样本的真实房价与预测房价之间的均方误差来对“准确”进行刻画。
$$
\begin{cases}
 J(w,b)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{({{y}^{(i)}}-{{{\hat{y}}}^{(i)}})}^{2}}}\\ 
  {{{\hat{y}}}^{(i)}}=h({{x}^{(i)}})=w^T{{x}^{(i)}}+b  
\end{cases}
\tag{3-3}
$$
其中，$m$表示样本数量； $x^{(i)}$表示第$i$个样本为一个列向量；$w$表示模型对应的参数也为一个列向量； $y^{(i)}$表示第$i$个房屋的真实价格； $y^{(i)}$表示第$i$ 个房屋的预测价格。

由式(3-3)可知，当函数$J(w,b)$取最小值时的参数$\hat{w}$和$\hat{b}$，就是要求的目标参数。为什么？因为当$J(w,b)$取最小值时就意味着此时所有样本的预测值与真实值之间的误差（Error）最小。如果极端一点，就是所有预测值都等同于真实值，那么此时的$J(w,b)$就是0了。因此，对于如何求解模型 $h(x)$ 的问题就转换成了如何最小化函数$J(w,b)$的问题，而$J(w,b)$也有一个专门的术语叫作目标函数（Objective Function）或者代价函数（Cost Function）抑或损失函数（Loss Function）。关于目标函数的求解问题将在<font color = red>第XXXXXX节</font>内容中进行介绍。

## 3.1.2 梯形面积预测

在3.1.1.2小节的内容中，笔者分别介绍了单变量线性回归和多变量线性回归，接下来我们开始介绍多项式回归。那什么是多项式回归呢？现在假定已知矩形的面积公式，而不知道求解梯形的面积公式，并且同时手上有若干个类似图3-3所示的梯形。已知梯形的上底和下底，并且上底均等于高。现在需要建立一个模型，当任意给定一个类似图3-3中的梯形时能近似地算出其面积。面对这样的问题该如何进行建模呢？

<img src="https://moonhotel.oss-cn-shanghai.aliyuncs.com/images/000103.png" style="zoom:50%;" />

<center>
    图 3-3. 梯形块儿
</center>

### 3.1.2.1 多项式回归建模

首先需要明确的是，即使直接建模成类似于3.1.1.2节中的多变量线性回归模型$h(x)=w_1x_1+w_2x_2+b$也是可以的，只是效果可能不会太好。现在我们来分析一下，对于这个梯形，左边可以看成正方形，所以可以人为地构造第3个特征$(x_1)^2$，而整体也可以看成长方形的一部分，则又可以人为地构造出$x_1x_2$这个特征，最后，整体还可以看成大正方形的一部分，因此还可以构造出$(x_2)^2$这个特征。

根据上述内容可知，我们建模时除了以$x_1,x_2$作为特征外，还人为构造了$x_1x_2,x^2_1,x^2_2$这三个特征，并且后三个特征也存在着一定意义上的可解释性。因此，对于这么一个模型，我们也可以通过类似图3-4的方式进行表示：

<img src="https://moonhotel.oss-cn-shanghai.aliyuncs.com/images/000104.png" style="zoom:70%;" />

<center>
    图 3-4. 梯形面积测线性回归结构图（偏置未画出）
</center>

此时，我们便可以建立一个如式(3-4)所示的模型
$$
h(x)=x_1w_1+x_2w_2+(x_1)^2w_3+x_1x_2w_4+(x_2)^2w_5+b\tag{3-4}
$$
此时有读者可能会问，式(3-4)中有的部分重复累加了，计算出来的面积岂不大于实际面积吗？这当然不会，因为每一项前面都有一个权重参数$w_i$做系数，只要这些权重有正有负，就不会出现大于实际面积的情况。同时，可以发现$h(x)$中包含了$x_1x_2$、$(x_1)^2$、$(x_2)^2$这些项，因此将其称为多项式回归（Polynomial Regression）。

但是，只要进行如下替换，便可回到普通的线性回归： 
$$
h(x)={{x}_{1}}{{w}_{1}}+{{x}_{2}}{{w}_{2}}+{{x}_{3}}{{w}_{3}}+{{x}_{4}}{{w}_{4}}+{{x}_{4}}{{w}_{5}}+b\tag{3-5}
$$
其中，$x_3=(x_1)^2$、$x_4=x_1x_2$、$x_5=(x_2)^2$，只是在实际建模时先要将原始两个特征的数据转化为5个特征的数据，同时在正式进行预测时，向模型$h(x)$输入的也将是包含5个特征的数据。

### 3.1.2.2 从特征输入到特征提取

从图3-4可以看出，尽管我们是用了5个特征作为线性回归的特征输入进行建模，但是其原始特征依旧只有$x_1,x_2$这两个。而其余的三个只是我们人为构造的，其本质就相当于我们首先以人为的方式对原始输入进行了一次特征提取，然后再以提取后的特征来进行建模。那既然如此，我们可不可以通过图3-5所示的结构图来进行建模呢？

<img src="https://moonhotel.oss-cn-shanghai.aliyuncs.com/images/000105.png" style="zoom:80%;" />

<center>
    图 3-5. 梯形面积预测结构图（偏置并未画出）
</center>
其中左边的圆圈表示原始的输入特征，中间的圆圈表示对原始特征提取后的特征，右边圆圈表示最终的预测输出。

通过图3-5我们可以知道，该结构首先以$x_1,x_2$为基础进行特征提取得到3个不同的特征，然后再以这3个特征来建立了一个线性回归模型进行预测输出。因此，$\hat{y}$可以表示为：
$$
\begin{aligned}
a&=w_1x_1+w_2x_2+b_1\\[1ex]
b&=w_3x_1+w_4x_2+b_2\\[1ex]
c&=w_5x_1+w_6x_2+b_3\\[1ex]
\hat{y}&=w_7a+w_8b+w_9c+b_4
\end{aligned}\tag{3-6}
$$

那以图3-5所示的方式进行建模和图3-4所示的方式差别在哪儿呢？差别倒是有很多，但最大的差别在于构造特征的可解释性。也就是说，人为构造的特征一般都具有一定的可解释性，知道每个特征的含义（例如上面的$x_1x_2,x^2_1,x^2_2$）。而以图3-5中的方式得到的特征在我们直观看来并不具有可解释性（例如上面$a,b,c$这3个特征）。因此，在传统的机器学习中还专门有一个分支叫特征工程，即人为的根据原始特征来构造一系列可解释性的新特征。

那说一千道一万，到底能不能用式(3-6)的描述来进行建模呢？很遗憾，并不能。为什么呢？

根据式(3-6)可得：
$$
\begin{aligned}
\hat{y} 
&= w_7(w_1x_1+w_2x_2+b_1)+w_8(w_3x_1+w_4x_2+b_2)+w_9(w_5x_1+w_6x_2+b_3)+b_4\\[1ex]
&=(w_1w_7+w_3w_8+w_5w_9)x_1+(w_2w_7+w_4w_8+w_6w_9)x_2+w_7b_1+w_8b_2+w_9b_3+b_4\\[1ex]
&=\alpha x_1+\beta x_2+\gamma
\end{aligned}\tag{3-7}
$$
由此可知，根据式(3-7)的描述，建模得到的仍旧只是一个以原始特征$x_1,x_2$为输入的线性回归模型。那图3-5这么好的结构设想难道就这么放弃？当然不会，图3-5的结构并没错，错的是式子(3-6)中的描述。

### 3.1.2.3 从线性输入到非线性变换

在上面我们说到，如果以式(3-6)中的描述进行建模，那么我们最终得到的仍旧只是一个简单的线性回归模型，其原因在于，通过式子(3-6)我们得到的3个特征$a,b,c$仅仅只是$x_1,x_2$之间在不同权重下的线性组合。也就是说$a,b,c$都是3个线性的特征，如果再将其进行线性组合作为输出，那么整个模型仍旧只是原始特征的线性组合，并没有增加模型的复杂度。那该怎么办呢？既然一切都是“线性”的错，那么唯有抛弃“线性”引入非线性才是解决问题的正道，而所谓非线性即是指通过一个非线性函数对原始输出进行一次变换。

如式(3-8)所示，我们只需要对$a,b,c$这3个特征再进行一次非线性变换，那么整个模型也就不可能再被化简为线性了，因此所有问题也将迎刃而解。
$$
\begin{aligned}
a&=g(w_1x_1+w_2x_2+b_1)\\[1ex]
b&=g(w_3x_1+w_4x_2+b_2)\\[1ex]
c&=g(w_5x_1+w_6x_2+b_3)\\[1ex]
\hat{y} &=w_7a+w_8b+w_9c+b_4
\end{aligned}\tag{3-8}
$$
其中$g(\cdot)$为非线性的变换操作，称之为激活函数，例如常见的sigmoid函数，这部分内容将在<font color = red>第xxx节中</font>进行介绍。

## 3.1.3 深度神经网络

经过以上内容的介绍其实已经在不知不觉中将大家代入到了深度学习（Deep Learning）的领域中。所谓深度学习即是值构建一个多层神经网络（Neural Network），然后进行参数学习的过程，而“深度”只是多层神经网络的一个别称而已。因此，你还可以将深度学习称作是多层神经网络学习。

### 3.1.3.1 单层神经网络

如图3-6所示的线性回归模型就是一个简单的神经网络结构图，其中每个圆圈表示一个神经元（Neural），输入层神经元的个数表示数据集的特征维度，输出层神经元的个数表示输出维度。并且尽管这里有输入层和输出层两层，但是按照惯例我们只将含有权重参数的层称为一个网络层，因此线性回归模型是一个单层的神经网络。

<img src="https://moonhotel.oss-cn-shanghai.aliyuncs.com/images/000106.png" style="zoom:70%;" />

<center>
    图 3-6. 单层神经网络结构图（偏置未画出）
</center>
<font color = red>此图重新绘制，改为输入神经元为4个，输出改为2个</font>

同时，需要注意的是在图3-6所示的网络结构中，输出层的所神经元和输入层的所有神经元都是完全连接。在深度学习中，如果某一层每个神经元的输入都完全依赖于上一层所有神经元的输出，那么我们就将该层称作是一个全连接层（Fully-connected Layer）或者是稠密层（Dense Layer）。例如图3-6中的输出层就是一个全连接层。

### 3.1.3.2 深度神经网络

所谓深度神经网络就是指网络网络层数大于2的神经网络，如图3-7所示便是一个简单的深度神经网络，其包含有3个全连接层。同时，我们将输入层与输出层之间的所有层都称为隐藏层或隐含层（Hidden Layer）。

<img src="https://moonhotel.oss-cn-shanghai.aliyuncs.com/images/000107.png" style="zoom:70%;" />

<center>
    图 3-7. 深度神经网络结构图（偏置未画出）
</center>

<font color = red>此图重新绘制，改为输入神经元为4个，输出改为2个</font>

这里值得注意的是，通过上面房价预测和梯形面积预测这两个例子的介绍我们可以知道，对于输出层之前的所有层，我们都可以将其看成是一个特征提取的过程，而且越靠近输出层的隐含层也就意味着提取得到的特征越抽象。当原始输入经过多层网络的特征提取后，我们就可以将提取得到的特征输入到最后一层进行相应的操作（分类或者回归等）。

到此，对于什么是深度学习以及深度学习的理念就介绍完了，这一点在<font color = red>第xxx节中</font>我们还会再次提及。

## 3.1.4 线性回归简洁实现

经过前面3个小节内容的介绍我们对于深度学习的基本概念已经有了一定的了解，接下来笔者将开始介绍如何借助PyTorch框架来快速实现上面介绍的房价预测和梯形面积预测这两个实际示例。

### 3.1.4.1 PyTorch使用介绍

在正式介绍模型实现之前，我们先来看看即将需要使用到的PyTorch中相关模型接口的使用方法。

**1\. `nn.Linear()`使用**

根据图3-7可知，对于每个网络层来说均是一个全连接层，且都可以看成由多个线性组合构成。例如对于第1个全连接层来说，其输入维度为原始样本的特征维度数4，输出维度为5，即由5个线性组合构成了该全连接层。此时，我们可以通过如下方式来定义该全连接层，示例代码如下所示：

```python
import torch.nn as nn
layer = nn.Linear(4, 5) 
```

在上述代码中，第1行表示导入`torch`中的`nn`模块；第2行表示定义一个全连接层，且该全连接层的输入特征（神经元）数量为4，输出特征数量为5，且`nn.Linear()`内部已经自动随机初始化了网络层对应的权重参数。同理，对于第2个全连接层来说，其定义方式为`nn.Linear(5, 3) `。因此对于式(3-1)中的单变量线性回归来说，其定义方式为`nn.Linear(1, 1) `。

接着，我们便可以通过如下方式来完成一次全连接层的计算，示例代码如下所示：

```python
def test_linear():
    x = torch.tensor([[1., 2, 3, 4], [4, 5, 6, 7]], dtype=torch.float32)  # [2,4]
    layer = nn.Linear(4, 5)  #
    y = layer(x)
```

在上述代码中，第2行表示定义输入样本，形状为`[2,4]`列，即样本数量为2，特征数量为4；第4行则是计算该全连接层对应的结果，输出形状为`[2,5]`。

**2\. `nn.Sequential()`使用**

此时我们已经知道了如何定义一个全连接层并完成对应的计算过程，但现在出现的一个问题是图3-7中有多个全连接网络层，该如何定义并完成整个计算过程呢？一种最接的办法就是逐层单独定义并完成相应的计算过程，示例代码如下所示：

```python
def multi_layers():
    x = torch.tensor([[1., 2, 3, 4], [4, 5, 6, 7]], dtype=torch.float32)
    layer1 = nn.Linear(4, 5)
    layer2 = nn.Linear(5, 3)
    layer3 = nn.Linear(3, 1)
    y1 = layer1(x)
    y2 = layer2(y1)
    y3 = layer3(y2)
    print(y3)
```

但这样的写法会略显冗余，因为对于整个计算过程来说，我们几乎很少会用到中间结果，因此可以采用省略的写法。在PyTorch中，可以通过将所有的网络层放入到一个有序的容器中，然后一次完成整个计算过程，示例代码如下所示：

```python
def multi_layers_sequential():
    x = torch.tensor([[1., 2, 3, 4], [4, 5, 6, 7]], dtype=torch.float32)  # [2,4]
    net = nn.Sequential(nn.Linear(4, 5), nn.Linear(5, 3), nn.Linear(3, 1))
    y = net(x)
    print(y)
```

在上述代码中，第3行中`nn.Sequential()`便是这个有序容器，通过它可以便可以完成整个3层网络的计算过程。

**3\. `nn.MSELoss()`使用**

根据第3.1.1.3节内容可知，在定义好一个模型之后便需要通过最小化对应的损失函数来求解得到模型对应的权重参数。在此处，我们可以通过计算预测值与真实值之间的均方误差来构造损失函数。在PyTorch中，我们可以借助`nn.MSELoss()`来实现这一目的，示例代码如下所示：

```python
def test_loss():
    y = torch.tensor([1, 2, 3], dtype=torch.float32)
    y_hat = torch.tensor([2, 2, 1], dtype=torch.float32)
    l1 = 0.5 * torch.mean((y - y_hat) ** 2) 
    loss = nn.MSELoss(reduction='mean')
    l2 = loss(y, y_hat)
    print(l1,l2)
```

在上述代码中，第2~3行表示定义真实值和预测值这两个张量；第4行表示自行实现式(3-3)中的损失计算；第5~6行表示借助PyTorch中的`nn.MSELoss()`来进行实现，其中`reduction='mean'`表示返回均值，而`reduction='sum'`表示返回和。

在上述代码运行结束后便可以得到如下结果：

```python
tensor(0.8333) tensor(1.6667)
```

可以发现两者并不相等，其原因在于`nn.MSELoss()`在计算损失时并没有乘以0.5这个系数，不过两者本质上并没有区别。至于式(3-3)中为什么需要乘以0.5这个系数将在<font color = red>第xxx节中</font>进行介绍。

上述示例代码可参见[Code/Chapter03/C01_OP/main.py](https://github.com/moon-hotel/DeepLearningWithMe/blob/master/Code/Chapter03/C01_OP/main.py)文件。

### 3.1.4.2 房价预测实现

在熟悉了`nn.Linear()`和`nn.MSELoss()`这两个模块的基本使用方法后，我们再来看如何借助PyTorch快速实现房价预测这个线性回归模型。完整示例代码可参见[Code/Chapter03/C02_HousePrice/main.py](https://github.com/moon-hotel/DeepLearningWithMe/blob/master/Code/Chapter03/C02_HousePrice/main.py)文件。

**1\. 构建数据集**

首先需要构造后续使用到的数据集，实现代码如下所示：

```python
def make_house_data():
    np.random.seed(20)
    x = np.random.randn(100, 1) + 5  # 面积
    noise = np.random.randn(100, 1)
    y = x * 2.8 - noise  # 价格
    x = torch.tensor(x, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.float32)
    return x, y
```

在上述代码中，第2行为设置一个固定的随机种子，来使得每次产生样本保持一样；第3~5行表示随机产生100个样本点并加入相应的噪音，其中`x`表示房屋面积，`y`表示房屋价格；第6~7行表示将其转换为PyTorch中的张量，且指定类型为浮点型。第8行表示返回测试数据，两者的形状均为`[100,1]`。

**2\. 构建模型**

在构建完成数据集之后便需要构造整个单变量线性回归模型，实现代码如下所示：

```python
def train(x, y):
    input_node = x.shape[1]
    output_node = 1
    net = nn.Sequential(nn.Linear(input_node, output_node))
    loss = nn.MSELoss()  # 定义损失函数
    optimizer = torch.optim.SGD(net.parameters(), lr=0.003)  # 定义优化器
    for epoch in range(40):
        logits = net(x)
        l = loss(logits, y)
        optimizer.zero_grad()
        l.backward()
        optimizer.step()  # 执行梯度下降
        print("Epoch: {}, loss: {}".format(epoch, l))
    logits = net(x)
    return logits.detach().numpy()
```

在上述代码中，第2~3行表示分别指定模型的输入输出特征维度，其中`x.shape[1]`表示数据集的列数（特征维度数），这里得到的结果也是1；第4行则是先定义一个全连接层，然后再将其放入到序列容器中；第5行是定义网络的损失函数；第6行是定义随机梯度下降优化器来求解模型的权重参数，其中`net.parameters()`表示得到容器中所有网络层对应的参数，`lr`表示执行梯度下降时的学习率，关于梯度下降算法的具体原理将在下一小节中进行介绍；第7~13行则是迭代求解网络中的权重参数，并且整个迭代过程在深度学习中将其训练（Training），其中第8~12行也是今后所有模型训练的固定写法，各行代码的具体含义将在下节内容中逐一进行介绍；第14~15行则是根据训练完成的模型来对房价进行预测，并同时返回预测后的结果。

**3\. 可视化结果**

在得到模型的预测结果后，便可以借助`matplotlib`中的`plot`模型对其进行可视化，实现代码如下所示：

```python
def visualization(x, y, y_pred=None):
    plt.xlabel('面积', fontsize=15)
    plt.ylabel('房价', fontsize=15)
    plt.scatter(x, y, c='black')
    plt.plot(x, y_pred)
    plt.show()
    
if __name__ == '__main__':
    x, y = make_house_data()
    y_pred = train(x, y)
    visualization(x, y, y_pred)
```

在上述代码中，第2~3行用于指定横纵坐标的显示标签；第4行是对原始样本点进行可视化；第5行则是对预测结果进行可视化；第6行表示展示所有的绘制结果。最终可视化的结果如图3-8所示。

<div align=center>
<img width="400" src="../img/p3-8.jpg"/>
</div>

<center>
    图 3-8. 线性回归预测结果图
</center>

在图3-8中，圆点表示原始样本，直线表示模型根据输入面积预测得到的结果。

### 3.1.4.3 梯形面积预测实现

在介绍完上面线性回归的简洁实现示例后，对于第3.1.2节中梯形面积预测的实现过程就容易多了。完整示例代码可参见[Code/Chapter03/C03_Trapezoid/main.py](https://github.com/moon-hotel/DeepLearningWithMe/blob/master/Code/Chapter03/C03_Trapezoid/main.py)文件。

**1\. 构建数据集**

首先依旧是构建相应的梯形样本数据集，实现代码如下所示：

```python
def make_trapezoid_data():
    x1 = np.random.randint(5, 10, [50, 1]) / 10
    x2 = np.random.randint(10, 16, [50, 1]) / 10
    x = np.hstack((x1, x2))
    y = 0.5 * (x1 + x2) * x1
    x = torch.tensor(x, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.float32)
    return x, y
```

在上述代码中，第2~3行分别用于随机生成梯形的上底和下底，其中第2行表示在整数范围5到10之间生成一个形状为50行1列向量，并缩小了10倍（便于模型训练）；第4行表示将两列向量拼接在一起得到一个50行2列的矩阵；第5行表示计算梯形真实的面积；第6~8行是分别将`x`和`y`转换为PyTorch中的张量并返回。

**2\. 构建模型**

在构建完数据集之后便需要图3-5中的网络结构来构造整个多层神经网络模型，实现代码如下所示：

```python
def train(x, y):
    input_node = x.shape[1]
    losses = []
    net = nn.Sequential(nn.Linear(input_node, 50),nn.Sigmoid(),nn.Linear(50, 1))
    loss = nn.MSELoss()  
    optimizer = torch.optim.Adam(net.parameters(), lr=0.003)  # 定义优化器
    for epoch in range(1000):
        logits = net(x)
        l = loss(logits, y)
        optimizer.zero_grad()
        l.backward()
        optimizer.step()
        losses.append(l.item())
    logits = net(x)
    l = loss(logits, y)
    print("真实值：", y[:5].detach().numpy().reshape(-1))
    print("预测值：", logits[:5].detach().numpy().reshape(-1))
    return losses
```

在上述代码中，第4行表示定义整个2层的网络模型，且同时将隐藏层神经元的个数设定为了50并加入了sigmoid非线性变换；第6行是定义一个优化器来求解模型参数，<font color = red>关于Adam优化器将在第5章中进行介绍</font>；第13行则是将每一次迭代后模型的损失值进行保存，其中`item()`方法表示将PyTorch中的一个标量转换为Python中的标量，如果是向量则需要使用`.detach().numpy()`方法进行转换；第16~17行则是分别输出前5个真实值和预测值。

上述代码运行结束后便可以得到如下输出结果：

```python
真实值： [0.84  0.7   0.855 0.665 0.54 ]
预测值： [0.83861655 0.7117218  0.85532755 0.6754495  0.54401684]
```

从输出结果可以看出，模型的预测结果和真实值已经非常接近了。

最后，还可以对网络训练过程中保存的损失值进行可视化，如图3-9所示。

<div align=center>
<img width="400" src="../img/p3-9.jpg"/>
</div>

<center>
    图 3-9. 梯形面积预测损失图
</center>

从图3-9可以看出，模型大约在迭代600次之后便进行入了收敛阶段。

## 3.1.5 梯度下降算法

经过第3.1.4节内容的介绍我们已经知道了如何借助PyTorch中的优化器来求解得到网络模型对应的权重参数，不过对于整个求解过程的具体原理并没有介绍。根据第3.1.1.3节内容可知，求解网络模型参数的过程便是等价于最小化目标函数$J(w,b)$的过程。在接下来的这一节内容中，笔者将会详细介绍如何通过梯度下降算法来最小化目标函数$J(w,b)$。

### 3.1.5.1 梯度下降引例

根据上面的介绍可以知道，梯度下降算法的目的是用来最小化目标函数，也就是说梯度下降算法是一个求解的工具。当目标函数取到（或接近）全局最小值时，我们也就求解得到了模型所对应的参数。不过那什么又是梯度下降（Gradient Descent）呢？如图3-10所示，假设有一个山谷，并且你此时处于位置A处，那么请问以什么样的方向（角度）往前跳，你才能最快地到达谷底B处呢?

<div align=center>
<img width="400" src="../img/p2-9.png"/>
</div>

<center>
  图 3-10 跳跃方向
</center>
现在大致有3个方向可以选择，沿着$Y$轴的$\boldsymbol{V_1}$方向，沿着$X$轴的$\boldsymbol{V_2}$方向及沿着两者间的$\boldsymbol{l}$方向。其实不用问，各位读者一定都会选择$\boldsymbol{l}$所在的方向往前跳第一步，然后接着选类似的方向往前跳第二步直到谷底。可为什么都应该这样选呢？答： 这还用问一看就知，不信请读者自己试一试。

### 3.1.5.2 方向导数与梯度

由一元函数导数的相关知识可知，函数$f(x)$在$x_0$处的导数反映的是$f(x)$在$x=x_0$处时的变化率；$|f^{\prime}(x_0)|$越大，也就意味着$f(x)$在该处的变化率越大，即移动$\Delta x$后产生的函数增量$\Delta y$越大。同理，在二元函数$z=f(x,y)$中，为了寻找$z$在A处的最大变化率，就应该计算函数$z$在该点的方向导数
$$
\frac{\partial f}{\partial \boldsymbol{l}}=\{\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}\}\cdot \{cos\alpha ,cos\beta \}=|gradf|\cdot |\boldsymbol{l}|\cdot \cos \theta\tag{3-9}
$$
其中，$\boldsymbol{l}$为单位向量； $\alpha$和$\beta$分别为$\boldsymbol{l}$与$x$轴和$y$轴的夹角； $\theta$为梯度方向与$\boldsymbol{l}$的夹角。

根据式(3-9)可知，要想方向导数取得最大值，那么$\theta$必须为0。由此可知，只有当某点处方向导数的方向与梯度的方向一致时，方向导数在该点才会取得最大的变化率。

在图3-10中，已知$z=x^2+y^2+5$，A的坐标为$(-3,3,23)$，则，则$\partial z/\partial x=2x,\partial z/\partial y=2y$。由此可知，此时在点A处梯度的方向为$(-6,6)$，所以当你站在A点并沿各个方向往前跳跃同样大小的距离时，只有沿着$(\sqrt{2}/2,-\sqrt{2}/2)$这个方向（进行了单位化，并且同时取了相反方向，因为这里需要的是负增量）才会产生最大的函数增量$\Delta z$。

如图3-11所示，要想每次都能以最快的速度下降，则每次都必须向着梯度的反方向向前跳跃。

<div align=center>
<img width="400" src="../img/p2-10.png"/>
</div>

<center>
  图 3-11 负梯度方向
</center>

### 3.1.5.3 梯度下降原理

介绍这么多总算是把梯度的概念讲清楚了，那么如何用具体的数学表达式进行描述呢？总不能一个劲儿地喊它“跳”对吧。为了方便后面的表述及将读者带入一个真实求解的过程中，这里先将图3-10中的字母替换成模型中的参数进行表述。

现在有一个模型的目标函数$J(w_1,w_2)=w_1^2+w_2^2+2w_2+5$（为了方便可视化，此处省略了参数$b$，但是原理都一样），其中$w_1$和$w_2$为待求解的权重参数，并且随机初始化点A为初始权重值。下面就一步步地通过梯度下降算法进行求解。

如图3-12所示，设初始点$A=(w_1,w_2)=(-2,3)$，则此时$J(-2,3)=24$，并且点$A$第一次往前跳的方向为 $-grad\;J=-(2{{w}_{1}},2{{w}_{2}}+2)=(1,-2)$ ，即$(1,-2)$这个方向。

<div align=center>
<img width="400" src="../img/p2-11.png"/>
</div>

<center>
  图 3-12 梯度下降
</center>
如图3-13所示，$OQ$为平面上梯度的反方向，$AP$为其平移后的方向，但是长度为之前的$\alpha$倍，因此，根据梯度下降的原则，此时曲面上的$A$点就该沿着其梯度的反方向跳跃，而投影到平面则为$A$应该沿着$AP$的方向移动。假定曲面上从$A$点跳跃到了$P$点，那么对应在投影平面上就是图3-13中的$AP$部分，同时权重参数也从$A$的位置更新到了$P$点的位置。

<div align=center>
<img width="400" src="../img/p2-12.png"/>
</div>

<center>
  图 3-13 梯度计算
</center>
从图3-13可以看出，向量$\mathbf{AP}$、$\mathbf{OA}$和$\mathbf{OP}$三者的关系为
$$
\mathbf{OP}=\mathbf{OA}-\mathbf{PA}\tag{3-10}
$$
进一步，可以将式(3-10)改写成
$$
\mathbf{OP}=\mathbf{OA}-\alpha \cdot grad\ J\tag{3-11}
$$


又由于$\mathbf{OP}$和$\mathbf{OA}$本质上就是权重参数$w_1$和$w_2$更新后与更新前的值，所以便可以得出梯度下降的更新公式为

$$
w=w-\alpha \cdot \frac{\partial J}{\partial w}\tag{3-12}
$$

其中，$w=(w_1,w_2)$，$\partial J/\partial w$为权重的梯度方向； $\alpha$为步长，用来放缩每次向前跳跃的距离，即优化器中的学习率（Learning Rate）参数。

根据式(3-12)可以得出，对于待优化求解的目标函数$J(w)$来说，如果需要通过梯度下降算法来进行求解，那么首先需要做的便是得到目标函数关于未知参数的梯度，即$\partial J/\partial w$。各位读者一定要记住这一点，在下一节内容中我们也将会再次提及。

进一步，将式(3-12)代入具体数值后可以得出，曲面上的点A在第一次跳跃后的着落点为
$$
\begin{aligned}
  & {{w}_{1}}={{w}_{1}}-0.1\times 2\times {{w}_{1}}=-2-0.1\times 2\times (-2)=-1.6 \\ 
 & {{w}_{2}}={{w}_{2}}-0.1\times (2\times {{w}_{2}}+2)=3-0.1\times (2\times 3+2)=2.2 \\ 
\end{aligned}\tag{3-13}
$$
此时，权重参数便从$(-2,3)$更新为$(-1.6,2.2)$。当然其目标函数$J(w_1,w_2)$也从24更新为16.8。至此，笔者便详细地完成了1轮梯度下降的计算。当$A$跳跃到$P$之后，又可以再次利用梯度下降算法进行跳跃，直到跳到谷底（或附近）为止，如图3-14所示。

<div align=center>
<img width="500" src="../img/p2-13.png"/>
</div>

<center>
  图 3-14 梯度下降
</center>

最后，根据上述原理，还可以通过实际的代码将整个过程展示出来，完整代码见[Code/Chapter03/C04_GradientDescent/main.py](https://github.com/moon-hotel/DeepLearningWithMe/blob/master/Code/Chapter03/C04_GradientDescent/main.py)文件，梯度下降核心代码如下：

```python
def compute_gradient(w1, w2):
    return [2 * w1, 2 * w2 + 2]

def gradient_descent():
    w1, w2 = -2, 3
    jump_points = [[w1, w2]]
    costs,step = [cost_function(w1, w2)],0.1
    print("P:({},{})".format(w1, w2), end=' ')
    for i in range(20):
        gradients = compute_gradient(w1, w2)
        w1 = w1 - step * gradients[0]
        w2 = w2 - step * gradients[1]
        jump_points.append([w1, w2])
        costs.append(cost_function(w1, w2))
        print("P{}:({},{})".format(i + 1, round(w1, 3), round(w2, 3)), end=' ')
    return jump_points, costs
```

在上述代码中，第1~2行是返回目标函数关于参数的梯度；第5~6行是初始化起始点；第7行是计算初始损失值和定义学习率为0.1；第9~15行则是迭代整个梯度下降过程，迭代次数为20次，其中第11~12行便是执行式(3-12)中的计算过程；第16行则是返回最后得到的计算结果。

上述代码运行结束后便可以得到如下所示的结果：

```python
P:(-2,3) P1:(-1.6,2.2) P2:(-1.28,1.56) P3:(-1.024,1.048) P4:(-0.819,0.638) P5:(-0.655,0.311) 
P6:(-0.524,0.049) P7:(-0.419,-0.161) P8:(-0.336,-0.329) P9:(-0.268,-0.463) P10:(-0.215,-0.571)......
```

通过上述代码便可以详细展示跳向谷底时每一次的落脚点，并且可以看到谷底的位置就在$(-0.023,-0.954)$附近，如图3-15所示。

<div align=center>
<img width="500" src="../img/p2-14.png"/>
</div>

<center>
  图 3-15 梯度下降可视化
</center>

此致，笔者就介绍完了如何通过编码实现梯度下降算法的求解过程，等后续我们再来自己编码从零完成网络模型的参数求解过程。

## 3.1.6 网络训练过程

在第3.14节内容中，当我们定义好损失函数后直接通过两行代码便完成了模型权重参数的优化求解过程，一句是`            l.backward()`，而另一句则是` optimizer.step()`。那这两句代码又是什么意思呢？同时，根据第3.1.5节内容可知，使用梯度下降求解模型参数的前提便是需要知道损失函数$J$关于权重的梯度，也就是要求得$J$关于模型中每个参数的偏导数。对于简单的单层神经网络我们倒是可以像第3.1.5节中那样自己手动计算每个权重参数的梯度，但是在深度学习中对于动则几十上百层的网络参数还能用手动的方式计算梯度吗？答案显然是否定的，一方面是自己推导容易出错且不易察觉，另一方面则是在实际建模过程中会频繁调整网络结构，试想一下如果每次变动网络结构都需要手动计算梯度，那工作量简直是难以想象。

在接下来这节内容中，笔者将会详细介绍深度学习中求解网络参数梯度的利器——反向传播（Back Propagation）算法，以及整个网络的详细训练过程。

### 3.1.6.1 前向传播过程

在具体介绍网络的训练过程前，笔者先来介绍网络训练结束后的整个预测过程。假定现在有如图3-16所示的一个网络结构图。

<img src="https://moonhotel.oss-cn-shanghai.aliyuncs.com/images/000174.png" style="zoom:60%;" />

<center>
    图 3-16. 网络结构图
</center>
此时定义：$L$ 表示神经网络总共包含的层数，$S_l$ 表示第$l$层的神经元数目，$K$ 表示输出层的神经元数目，$w_{ij}^l$ 表示第$l$层第$j$个神经元与第$l+1$层第$i$个神经元之间的权重值。

此时对于图3-16所示的网络结构来说，$L=3,S_1=3$，$S_2=4,S_3=K=2$，$a^l_i$表示第$l$层第$i$个神经元的激活值（输入层$a^1_i=x_i$，输出层$a^L_i=\hat{y}_i$），$b^l_i$表示第$l$层的第$i$个偏置（未画出）。

根据图3-16所示的网络结构图，当输入1个样本对其进行预测时，那么网络第1层的计算过程可以表示成如下形式
$$
\begin{aligned}
z^2_1&=a_1^1w^1_{11}+a^1_2w^1_{12}+a^1_3w^1_{13}+b^1_1\\[1ex]
z^2_2&=a_1^1w^1_{21}+a^1_2w^1_{22}+a^1_3w^1_{23}+b^1_2\\[1ex]
z^2_3&=a_1^1w^1_{31}+a^1_2w^1_{32}+a^1_3w^1_{33}+b^1_3\\[1ex]
z^2_4&=a_1^1w^1_{41}+a^1_2w^1_{42}+a^1_3w^1_{43}+b^1_4
\end{aligned}\tag{2-3-14}
$$
如果是将其以矩阵的形式进行表示，则式(3-14)可以改写为
$$
\begin{bmatrix}z^2_1\\ z^2_2\\z^2_3\\z^2_4\end{bmatrix}^T
=\begin{bmatrix}a^1_1&a^1_2&a^1_3\end{bmatrix}_{1\times3}\times \begin{bmatrix}
w^1_{11}&w^1_{21}&w^1_{31}&w^1_{41}\\
w^1_{12}&w^1_{22}&w^1_{32}&w^1_{42}\\
w^1_{13}&w^1_{23}&w^1_{33}&w^1_{43}\\
\end{bmatrix}_{3\times4}+\begin{bmatrix}b^1_1\\b^1_2\\b^1_3\\b^1_4\end{bmatrix}^T\tag{3-3-15}
$$
进一步，将式(3-15)中的形式进行简化可以得出
$$
z^2=a^1w^1+b^1\implies a^2=f(z^2)\tag{4-3-16}
$$
其中，$f(\cdot)$表示激活函数，如sigmoid函数等。

同理对于第2层来说有
$$
z^3=a^2w^2+b^2\implies a^3=f(z^3)\tag{3-17}
$$
现在如果用一个通式对(3-17)进行表示的话则为
$$
\begin{aligned}
z^{l+1}_i&=a^l_1w^l_{i1}+a^l_2w^l_{i2}+\cdots+a^l_{S_l}w^l_{iS_l}+b^l\\[1ex]
z^{l+1}&=a^lw^l+b^l\\[1ex]
a^{l+1}&=f(z^{l+1})
\end{aligned}\tag{5-3-18}
$$

由此可以发现，上述整个计算过程，从输入到输出是根据从左到右按序计算而得到，因此，整个计算过程又被形象的叫做正向传播（Forward Propagation）或者是前向传播。

现在我们已经知道了什么正向传播过程，即当我们训练得到权重参数$w$之后便可以使用正向传播来进行预测了。进一步，我们再来看如何求解目标函数关于权重参数的梯度，以便通过梯度下降算法进行求解网络参数。

### 3.1.6.2 传统方式梯度求解

以图3-16所示的网络结构为例，假设网络的目标函数为均方误差损失，且同时只考虑一个样本即
$$
J(w,b)=\frac{1}{2}({y}-{\hat{y}})^{2}\tag{8-3-19}
$$
其中，$w$表示整个网络中的所有权重参数，$b$表示所有的偏置，$\hat{y}=a^3$。

由此根据图3-16可以发现，如果$J$对$w^1_{11}$求导，则$J$是关于$a^3$的函数，$a^3$是关于$z^3$的函数，$z^3$是关于$a^2$的函数，$a^2$是关于$z^2$的函数，$z^2$是关于$w^1_{11}$的函数。

所以根据链式求导法则有
$$
\begin{aligned}
\frac{\partial J}{\partial w^1_{11}}&=\frac{\partial J}{\partial a^3_{1}}\cdot\frac{\partial a^3_{1}}{\partial z^3_{1}}\cdot\frac{\partial z^3_{1}}{\partial a^2_1}\cdot\frac{\partial a^2}{\partial z^2_1}\cdot\frac{\partial z^2_1}{\partial w^1_{11}}+\frac{\partial J}{\partial a^3_{2}}\cdot\frac{\partial a^3_{2}}{\partial z^3_{2}}\cdot\frac{\partial z^3_{2}}{\partial a^2_1}\cdot\frac{\partial a^2}{\partial z^2_1}\cdot\frac{\partial z^2_1}{\partial w^1_{11}}\\[3ex]
\frac{\partial J}{\partial w^1_{12}}&=\frac{\partial J}{\partial a^3_{1}}\cdot\frac{\partial a^3_{1}}{\partial z^3_{1}}\cdot\frac{\partial z^3_{1}}{\partial a^2_1}\cdot\frac{\partial a^2}{\partial z^2_1}\cdot\frac{\partial z^2_1}{\partial w^1_{12}}+\frac{\partial J}{\partial a^3_{2}}\cdot\frac{\partial a^3_{2}}{\partial z^3_{2}}\cdot\frac{\partial z^3_{2}}{\partial a^2_1}\cdot\frac{\partial a^2}{\partial z^2_1}\cdot\frac{\partial z^2_1}{\partial w^1_{12}}\\
&\vdots\\
\frac{\partial J}{\partial w^2_{22}}&=\frac{\partial J}{\partial a^3_2}\cdot\frac{\partial a^3_2}{\partial z^3_2}\cdot\frac{\partial z^3_2}{\partial w^2_{22}}
\end{aligned}\tag{9-3-20}
$$
根据式(3-20)可以发现，当目标函数$J$对第2层的参数如$w^2_{22}$求导还相对不太麻烦，但当$J$对第1层的参数进行求导时，就做了很多重复的计算，并且这还是网络相对简单的时候，要是网络相对复杂一点，这个过程便会无从下手。显然这种求解梯度的方式是不可取的，这也是为什么神经网络在一段时间发展缓慢的原因，就是因为没有一种高效的计算梯度的方式。

### 3.1.6.3 反向传播过程

由式(3-20)中第1行可知，我们可以将其整理成如下形式
$$
\begin{aligned}
\frac{\partial J}{\partial w^1_{11}}&=\left(\frac{\partial J}{\partial a^3_{1}}\cdot\frac{\partial a^3_{1}}{\partial z^3_{1}}\cdot\frac{\partial z^3_{1}}{\partial a^2_1}\cdot\frac{\partial a^2}{\partial z^2_1}\right)\cdot\frac{\partial z^2_1}{\partial w^1_{11}}+\left(\frac{\partial J}{\partial a^3_{2}}\cdot\frac{\partial a^3_{2}}{\partial z^3_{2}}\cdot\frac{\partial z^3_{2}}{\partial a^2_1}\cdot\frac{\partial a^2}{\partial z^2_1}\right)\cdot\frac{\partial z^2_1}{\partial w^1_{11}}
\end{aligned}\tag{10-3-21}
$$
从式(3-21)可以看出，不管是从哪一条路径过来，在对$w^1_{11}$求导之前都会先到达$z^2_1$，即先对$z^2_1$求导之后，才会有$\partial z^2_1/\partial w^1_{11}$。由此可以得出，不管之前是经过什么样的路径到达$w^l_{ij}$，在对连接第$l$层第$j$个神经元与第$l+1$第$i$个神经元的参数$w^l_{ij}$求导之前，肯定会先对$z^{l+1}_i$求导。因此，对任意参数的求导过程可以改写为
$$
\frac{\partial J}{\partial w^l_{ij}}=\frac{\partial J}{\partial z^{l+1}_i}\cdot\frac{\partial z^{l+1}_i}{\partial w^l_{ij}}=\frac{\partial J}{\partial z^{l+1}_i}\cdot a^l_j\tag{11-3-22}
$$
例如：
$$
\frac{\partial J}{\partial w^1_{11}}=\frac{\partial J}{\partial z^{1+1}_1}\cdot\frac{\partial z^{1+1}_1}{\partial w^1_{11}}=\frac{\partial J}{\partial z^2_1}\cdot\frac{\partial z^2_1}{\partial w^1_{11}}=\frac{\partial J}{\partial z^2_1}\cdot a^1_1\tag{12-3-23}
$$
所以，现在的问题变成了如何快速求解式(3-22)中的$\partial J/ \partial z^{l+1}_i$部分。

从图3-16所示的网络结构可以看出，目标函数$J$对任意$z^l_i$求导时，求导路径必定会经过第$l+1$层的所有神经元，于是结合式(3-18)有
$$
\begin{aligned}
\frac{\partial J}{\partial z^l_i}&=\frac{\partial J}{\partial z^{l+1}_1}\cdot\frac{\partial z^{l+1}_1}{\partial z^l_i}+\frac{\partial J}{\partial z^{l+1}_2}\cdot\frac{\partial z^{l+1}_2}{\partial z^l_i}+\cdots+\frac{\partial J}{\partial z^{l+1}_{S_{l+1}}}\cdot\frac{\partial z^{l+1}_{S_{l+1}}}{\partial z^l_i}\\[1ex]
&=\sum_{k=1}^{S_{l+1}}\frac{\partial J}{\partial z^{l+1}_k}\cdot\frac{\partial z^{l+1}_k}{\partial z^l_i}\\[1ex]
&=\sum_{k=1}^{S_{l+1}}\frac{\partial J}{\partial z^{l+1}_k}\cdot\frac{\partial}{\partial z^l_i}(a^l_1w^l_{k1}+a^l_2w^l_{k2}+\cdots+a^l_{S_l}w^l_{kS_l}+b^l)\\[1ex]
&=\sum_{k=1}^{S_{l+1}}\frac{\partial J}{\partial z^{l+1}_k}\cdot\frac{\partial}{\partial z^l_i}\sum_{j=1}^{S_l}a^l_jw^l_{kj}\\[1ex]
&=\sum_{k=1}^{S_{l+1}}\frac{\partial J}{\partial z^{l+1}_k}\cdot\frac{\partial}{\partial z^l_i}\sum_{j=1}^{S_l}f(z^l_j)w^l_{kj}\\[1ex]
&=\sum_{k=1}^{S_{l+1}}\frac{\partial J}{\partial z^{l+1}_k}\cdot f^{\prime}(z^l_{i})w^l_{ki}
\end{aligned}\tag{14-3-24}
$$
于是此时有
$$
\frac{\partial J}{\partial z^l_i}=\sum_{k=1}^{S_{l+1}}\frac{\partial J}{\partial z^{l+1}_k}\cdot f^{\prime}(z^l_i)w^l_{ki}\tag{15-3-25}
$$
进一步，根据式(3-25)可以推导得出
$$
\frac{\partial J}{\partial z^{l+1}_i}=\sum_{k=1}^{S_{l+2}}\frac{\partial J}{\partial z^{l+2}_k}\cdot f'(z^{l+1}_i)w^{l+1}_{ki}\tag{16-3-26}
$$
为了便于书写和观察规律，我们引入一个中间变量$\delta^l_i=\frac{\partial J}{\partial z^l_i}$，则式(3-24)可以重新写为
$$
\delta^l_i=\frac{\partial J}{\partial z^l_i}=\sum_{k=1}^{S_{l+1}}\delta^{l+1}_k\cdot f^{\prime}(z^l_i)w^l_{ki}\;,\;\;(l<=L-1)\tag{17-3-27}
$$
需要注意的是，之所以要$l<=L-1$，是因为由式(3-24)的推导过程可知，$l$最大只能取到$L-1$，因为第$L$层后面没有网络层了。

所以，当以均方误差为损失函数时有
$$
\begin{aligned}
\delta^{L}_i&=\frac{\partial J}{\partial z^L_{i}}=\frac{\partial }{\partial z^L_{i}}\frac{1}{2}\sum_{k=1}^{S_L}(\hat{y}_k-y_k)^2\\[1ex]
&=\frac{\partial }{\partial z^L_{i}}\frac{1}{2}\sum_{k=1}^{S_L}(f(z_k^L)-y_k)^2\\[ 1ex]
&=\left[f(z^L_i)-y_i\right]\cdot f^{\prime}(z^L_i)\\[2ex]
&=\left[a^L_i-y_i\right]\cdot f^{\prime}(z^L_i)
\end{aligned}\tag{18-3-28}
$$
根据式(3-28)可以看出，均方误差损失函数前面乘以0.5这个系数的目的便是在求导时能消除平方项，使整个式子看起来更简洁。

同时将式(3-27)带入式(3-22)可得
$$
\frac{\partial J}{\partial w^l_{ij}}=\delta^{l+1}_i\cdot a^l_j\tag{19-3-29}
$$
通过上面的所有推导，由此我们可以得到如下4个迭代公式
$$
\frac{\partial J}{\partial w^l_{ij}}=\delta^{l+1}_i\cdot a^l_j\tag{3-30}
$$

$$
\frac{\partial J}{\partial b^l_{i}}=\delta^{l+1}_i\cdot 1\tag{3-31}
$$

$$
\delta^l_i=\frac{\partial J}{\partial z^l_i}=\sum_{k=1}^{S_{l+1}}\delta^{l+1}_k\cdot f^{\prime}\tag{3-32}(z^l_i)w^l_{ki},\;\;(0 < l \leq L-1)
$$

$$
\delta^L_i=[a^L_i-y_i]\cdot f'(z^L_i)\tag{3-33}
$$

注：这里$\delta^L_i$的结果只是针对于损失函数为均方误差时的情况，如采用其它损失函数需根据式(3-28)的形式重新推导。

且式(3-30)~式(3-33)经过矢量化后的形式为
$$
\frac{\partial J}{\partial w^l}=(a^l)^T\otimes\delta^{l+1} \tag {20-3-34}
$$

$$
\frac{\partial J}{\partial b^l}=\delta^{l+1} \tag {3-35}
$$

$$
\delta^l=\delta^{l+1}\otimes(w^l)^T\odot f^{\prime}(z^l)\tag {21-3-36}
$$

$$
\delta^{L}=[a^L-y]\odot f^{\prime}(z^L)\tag {22-3-37}
$$

其中$\otimes$表示矩阵乘法，$\odot$表示按位乘操作。

由式(3-34)~式(3-37)分析可知，欲求$J$对$w^l$的导数，必先知道$\delta^{l+1}$；而欲知$\delta^{l+1}$，必先求$\delta^{l+2}$，以此类推。由此可知对于整个求导过程，一定是先求$\delta^L$，再求$\delta^{L-1}$，一直到$\delta^{2}$。

因此，对于图3-16这样一个网络结构，整个梯度求解过程为先根据式(3-37)求解得到$\delta^{3}$；然后根据式(3-34)和式(3-35)分别求得$\partial J/\partial w^2$和$\partial J/\partial b^2$的结果；接着再根据式(3-36)并依赖$\delta^{3}$求解得到$\delta^{2}$的结果；最后再根据式(3-34)和式(3-35)分别求得$\partial J/\partial w^1$和$\partial J/\partial b^1$的结果。

此时，我们终于发现了这么一个不争的事实：①最先求解出偏导数的参数一定位于第$L-1$层上（如此处的$w^2$）；②要想求解第$l$层参数的偏导数，一定会用到第$l+1$层上的中间变量$\delta^{l+1}$（如此处求解$w^1$的导数，用到了$\delta^2$）；③整个过程是从右往左依次进行。所以，整个从右到左的计算过程又被形象地称为反向传播（Back Propagation），且$\delta^l$被称为第$l$层的“残差”（Residual）。

最后，在通过整个反向传播过程计算得到所有权重参数的梯度后，便可以根据式(3-12)中的梯度下降算法进行参数更新，而这两个计算过程对应的便是本节内容一开始所提到的`l.backward()`和`optimizer.step()`这两个操作。同时，笔者需要再次强调的是，梯度下降算法的作用是用来最小化目标函数求解网络参数，而使用梯度下降算法的前提便是要知道所有参数相应的梯度，而反向传播算法正是一种高效的求解梯度的工具，千万不要把两种混为一谈。

## 3.1.7 从零实现多层神经网络



## 3.1.8 小结
