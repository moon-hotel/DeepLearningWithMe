# 3.10 过拟合与正则化

经过前面几节内容的介绍， 我们对于深度学习的理念以及最基本的回归和分类模型已经有了清晰的认识。在接下来的这节内容中，笔者将逐步开始介绍深度学习中关于模型优化的一些基本内容，包括模型的过拟合、正则化和丢弃法等。

## 3.10.1 模型拟合

在第3.3节内容中，笔者首次引入了梯度下降这一优化算法，以此来最小化线性回归中的目标函数，并且在经过多次迭代后便可以得到模型中对应的参数。此时可以发现，模型的参数是一步一步根据梯度下降算法更新而来，直至目标函数收敛，也就是说这是一个循序渐进的过程，因此，这一过程也被称作拟合（Fitting）模型参数的过程，当这个过程执行结束后就会产生多种拟合后的状态，例如过拟合（Overfitting）和欠拟合（Underfitting）等。

在第3.8节内容中，笔者介绍了几种评估回归模型常用的指标，但现在有一个问题： 当MAE或者RMSE越小时就代表模型越好吗？还是说在某种条件下其越小就越好呢？细心的读者可能一眼便明了，肯定是有条件下的越小所对应的模型才越好。那这其中到底是怎么回事呢？

假设现在有一批样本点，它本是由函数$sin(x)$生成（现实中并不知道），但由于其他因素的缘故，使我们得到的样本点并没有准确地落在曲线$sin(x)$上，而是分布在其附近，如图3-38所示。

<div align=center>
<img width="380" src="../img/23020633449.jpg"/>
</div>

<center>
  图 3-38 正弦样本点图形
</center>

如图3-38所示，黑色圆点为训练集，黑色曲线为样本真实的分布曲线。现在需要根据训练集来建立并训练模型，然后得到相应的预测函数。现在我们分别用3个不同的模型A、B和C（复杂度依次增加，例如更多的网络层数和神经元个数等）来分别根据这12个样本点进行建模，那么最终便可以得到如图3-39所示的结果。

<div align=center>
<img width="380" src="../img/23020643127.jpg"/>
</div>

<center>
  图3-39正弦样本点拟合图形
</center>

从图3-39中可以看出，随着模型复杂度的增加，$R^2$指标的值也越来越大（$R^2\in(-\infty,1]$），并且在模型C中$R^2$还达到了1.0，但是最后就应该选择模型C吗？

不知到又过了多久，突然一名客户要买你的这个模型进行商业使用，同时客户为了评估这个模型的效果自己又带来了一批新的含有标签的数据（虽然模型C已经用$R^2$测试过，但客户并不会完全相信，万一你对这个模型作弊呢）。于是你拿着客户的新数据（也是由$sin(x)$所生成），然后分别用上面的3个模型进行预测，并得到了如图3-40所示的可视化结果。

<div align=center>
<img width="380" src="../img/23020658968.jpg"/>
</div>

<center>
  图 3-40 正弦样本点过拟合图形
</center>
如图3-40所示，各个曲线表示根据新样本预测值绘制得到的结果。此时令你感到奇怪的是，为什么模型B的结果居然会好于模型C的结果，问题出在哪里？其原因在于，当第1次通过这12个样本点进行建模时，为了尽可能地使“模型好（表现形式为$R^2$尽可能大）”而使用了非常复杂的模型，尽管最后每个训练样本点都“准确无误”地落在了预测曲线上，但是这却导致最后模型在新数据上的预测结果严重地偏离了其真实值。

## 3.10.2 过拟合与欠拟合概念

在机器学习领域中，通常将建模时所使用的数据叫作训练集（Training Dataset），例如图3-38中的12个样本点。将测试时所使用的数据集叫作测试集（Testing Dataset）。同时把模型在训练集上产生的误差叫作训练误差（Training Error），把模型在测试集上产生的误差叫作泛化误差（Generalization Error），最后也将整个拟合模型的过程称作训练（Training）[1]。

进一步讲，将3.10.1节中模型C所产生的现象叫作过拟合（Overfitting），即模型在训练集上的误差很小，但在测试集上的误差却很大，也就是泛化能力弱； 相反，将其对立面模型A所产生的现象叫作欠拟合（Underfitting），即模型训练集和测试集上的误差都很大； 同时，将模型B对应的现象叫作恰拟合（Goodfitting），即模型在训练集和测试集上都有着不错的效果。

同时，需要说明的是，在3.10.1节中笔者仅仅以回归任务为例来向读者直观地介绍了什么是过拟合与欠拟合，但并不代表这种现象只出现在回归模型中，事实上所有的深度学习模型都会存在着这样的问题，因此一般来讲，所谓过拟合现象指的是模型在训练集上表现很好，而在测试集上表现却糟糕，欠拟合现象是指模型在两者上的表现都十分糟糕，而过拟合现象是指模型在训练集上表现良好（尽管可能不如过拟合时好），但同时在测试集上也有着不错的表现。

## 3.10.3 解决欠拟合与过拟合

**1\. 如何解决欠拟合问题**

经过上面的描述我们已经对欠拟合有了一个直观的认识，所谓欠拟合就是训练出来的模型根本不能较好地拟合现有的训练数据。在深度学习中，要解决欠拟合问题相对来讲较为简单，主要分为以下3种方法： 

(1) 重新设计更为复杂的模型，例如增加网络的深度、神经元的个数或者采用更为复杂的网络架构（如Transformer）；

(2) 减小正则化系数，当模型出现欠拟合现象时，可以通过减小正则化中的惩罚系数来减缓欠拟合现象，这一点将在第3.10.4节中进行介绍。

**2\. 如何解决过拟合问题**

对于如何有效地缓解模型的过拟合现象，常见的做法主要分为以下4种方法： 

(1) 收集更多数据，这是一个最为有效但实际操作起来又是最为困难的一种方法。训练数据越多，在训练过程中也就越能够纠正噪声数据对模型所造成的影响，使模型不易过拟合，但是对于新数据的收集往往有较大的困难。

(2) 降低模型复杂度，当训练数据过少时，使用较为复杂的模型极易产生过拟合现象，例如3.10.1节中的示例，因此可以通过适当减少模型的复杂度来达到缓解模型过拟合的现象。

(3) 正则化方法，在出现过拟合现象的模型中加入正则化约束项，以此来降低模型过拟合的程度，这部分内容将在3.10.4节中进行介绍。

(4) 集成方法，将多个模型集成在一起，以此来达到缓解模型过拟合的目的。

**3\. 如何避免过拟合**

为了避免训练出来的模型产生过拟合现象，在模型训练之前一般会将获得的数据集划分成两部分，即训练集与测试集，且两者一般为7∶3的比例。其中训练集用来训练模型（降低模型在训练集上的误差），然后用测试集来测试模型在未知数据上的泛化误差，观察是否产生了过拟合现象 [2]。

但是由于一个完整的模型训练过程通常会先用训练集训练模型，再用测试集测试模型，而绝大多数情况下不可能第1次就选择了合适的模型，所以又会重新设计模型（如调整网络层数、正则化系数等）进行训练，然后用测试集进行测试，因此在不知不觉中，测试集也被当成了训练集在使用，所以这里还有另外一种数据的划分方式，即训练集、验证集（Validation Data）和测试集，且一般为7∶2∶1的比例，此时的测试集一般通过训练集和验证集选定模型后做最后测试所用。

实际训练中应该选择哪种划分方式呢？这一般取决于训练者对模型的要求程度。如果要求严苛就划分为3份，如果不那么严格，则可以划分为2份，也就是说这两者并没硬性的标准。

## 3.10.4 泛化误差的来源

根据第3.10.3节内容可以知道，模型产生过拟合的现象表现为在训练集上误差较小，而在测试集上误差较大，并且笔者还讲到，之所以会产生过拟合现象是由于训练数据中可能存在一定的噪声，而我们在训练模型时为了尽可能地做到拟合每个样本点（包括噪声），往往就会使用复杂的模型。最终使训练出来的模型在很大程度上受到了噪声数据的影响，例如真实的样本数据可能更符合一条直线，但是由于个别噪声的影响使训练出来的是一条曲线，从而使模型在测试集上表现糟糕，因此，可以将这一过程看作由糟糕的训练集导致了糟糕的泛化误差。但是，如果仅仅从过拟合的表现形式来看，糟糕的测试集（噪声多）也可能导致糟糕的泛化误差。

在接下来的内容中，笔者将分别从这两个角度来介绍正则化（Regularization）方法中最常用的$\mathcal{l} _2$正则化是如何来解决这一问题的。

这里以线性回归为例，我们首先来看一下在线性回归的目标函数后面再加上一个$\mathcal{l}_2$正则化项的形式。
$$
J=\sum\limits_{i=1}^{m}{{{\left[ {{y}^{(i)}}-(\sum\limits_{j=1}^{n}{{{w}_{j}}x_{j}^{(i)}}+b) \right]}^{2}}}+\frac{\lambda }{2n}\sum\limits_{j=1}^{n}{{{({{w}_{j}})}^{2}}};\;\;(\lambda >0)\tag{4-6,3-98}
$$
在式(3-98)中的第2项便是新加入的$\mathcal{l}_2$正则化项（Regularization Term），那它有什么作用呢？根据第3.1.3节中的内容可知，当真实值与预测值之间的误差越小（表现为损失值趋于0）时，也就代表着模型的预测效果越好，并且可以通过最小化目标函数来达到这一目的。由式(3-98)可知，为了最小化目标函数$J$，第2项的结果也必将逐渐地趋于0。这使最终优化求解得到的$w_j$均会趋于0附近，进而得到一个平滑的预测模型。这样做的好处是什么呢？

## 3.10.5 测试集导致的泛化误差

所谓测试集导致糟糕的泛化误差是指训练集本身没有多少噪声，但由于测试集含有大量噪声，使训练出来的模型在测试集上没有足够的泛化能力，而产生了较大的误差。这种情况可以看作模型过于准确而出现了过拟合现象。正则化方法是怎样解决这个问题的呢？
$$
y=\sum\limits_{j=1}^{n}{{{x}_{j}}}{{w}_{j}}+b\tag{4-7,3-99}
$$
假如式(3-99)所代表的模型就是根据式(3-98)中的目标函数训练而来的，此时当某个新输入样本（含噪声）的某个特征维度由训练时的$x_j$变成了现在的$(x_j+\Delta x_j)$，那么其预测输出就由训练时的$\hat{y}$变成了现在的$\hat{y}+\Delta x_jw_j$，即产生了$\Delta x_jw_j$的误差，但是，由于$w_j$接近于$0$附近，所以这使模型最终只会产生很小的误差。同时，如果$w_j$越接近于$0$，则产生的误差就会越小，这意味着模型越能够抵抗噪声的干扰，在一定程度上越能提升模型的泛化能力 [1]。

由此便可以知道，通过在原始目标函数中加入正则化项，便能够使训练得到的参数趋于平滑，进而能够使模型对噪声数据不再那么敏感，缓解了模型的过拟合现象。

## 3.10.6 训练集导致的泛化误差

所谓训练集导致糟糕的泛化误差是指，由于训练集中包含了部分噪声，导致我们在训练模型的过程中为了能够尽可能地最小化目标函数而使用了较为复杂的模型，使最终得到的模型并不能在测试集上有较好的泛化能力（如第3.10.1节中的示例），但这种情况完全是因为模型不合适而出现了过拟合的现象，而这也是最常见的过拟合的原因。$\mathcal{l}_2$正则化方法又是怎样解决在训练过程中就能够降低对噪声数据的敏感度的呢？为了便于后面的理解，我们先从图像上来直观地理解一下正则化到底对目标函数做了什么。

如图3-41所示，左右两边黑色实线为原始目标函数，黑色虚线为加了$\mathcal{l}_2$正则化后的目标函数。可以看出黑色实线的极值点均发生了明显改变，并且不约而同地都更靠近原点。

<div align=center>
<img width="550" src="../img/p4-14.png"/>
</div>

<center>
  图 3-41 $\mathcal{l}_2$正则化图形
</center>
再来看一张包含两个参数的目标函数在加入$\mathcal{l}_2$正则化后的结果，如图3-42所示。

<div align=center>
<img width="350" src="../img/p4-15.png"/>
</div>

<center>
  图 3-42 $\mathcal{l}_2$正则化投影图形
</center>
如图3-42所示，图中黑色虚线为原始目标函数的等高线，黑色实线为施加正则化后目标函数的等高线。可以看出，目标函数的极值点同样也发生了变化，从原始的$(0.5,0.5)$变成了$(0.0625,0.25)$，而且也更靠近原点（$w_1$和$w_2$变得更小了）。到此我们似乎可以发现，正则化能够使原始目标函数极值点发生改变，并且同时还有使参数趋于0的作用。事实上也正是因为这个原因才使$\mathcal{l}_2$正则化具有缓解过拟合的作用，但原因在哪里呢？

## 3.10.7 $\mathcal{l}_2$正则化原理

以目标函数${{J}_{1}}=1/6{{({{w}_{1}}-0.5)}^{2}}+{{({{w}_{2}}-0.5)}^{2}}$为例，其取得极值的极值点为$(0.5,0.5)$，且$J_1$在极值点处的梯度为$(0,0)$。当对其施加正则化$R=(w_1^2+w_2^2)$后，由于$R$的梯度方向是远离原点的（因为$R$为一个二次曲面），所以给目标函数加入正则化，实际上等价于给目标函数施加了一个远离原点的梯度。通俗点讲，正则化给原始目标函数的极值点施加了一个远离原点的梯度（甚至可以想象成施加了一个力的作用），因此，这也就意味着对于施加正则化后的目标函数$J_2=J_1+R$来讲，$J_2$的极值点$(0.0625,0.25)$相较于$J_1$的极值点$(0.5,0.5)$更加靠近于原点，而这也就是$\mathcal{l}_2$正则化本质之处。

注意：在通过梯度下降算法最小化目标函数的过程中，需要得到的是负梯度方向，因此上述极值点会向着原点的方向移动。

假如有一个模型$A$，它在含有噪声的训练集上表示异常出色，使目标函数$J_1(\hat{w})$的损失值等于$0$（也就是拟合到了每个样本点），即在$w=\hat{w}$处取得了极值。现在，我们在$J_1$的基础上加入$\mathcal{l}_2$正则化项构成新的目标函数$J_2$，然后来分析一下通过最小化$J_2$求得的模型$B$到底产生了什么样的变化。
$$
\begin{aligned}
  & {{J}_{1}}=\sum\limits_{i=1}^{m}{{{\left( {{y}^{(i)}}-(\sum\limits_{j=1}^{n}{x_{j}^{(i)}}{{w}_{j}}+b) \right)}^{2}}} \\[1ex] 
 & {{J}_{2}}={{J}_{1}}+\frac{\lambda }{2n}\sum\limits_{j=1}^{n}{{{({{w}_{j}})}^{2}}};\;\;(\lambda >0) \\ 
\end{aligned}\tag{4-8,3-100}
$$
从式(3-100)可知，由于$J_2$是由$J_1$加正则化项构成的，同时根据先前的铺垫可知，$J_2$将在离原点更近的极值点$w=\tilde{w}$处取得$J_2$的极值，即通过最小化含正则化项的目标函数$J_2$，将得到$w=\tilde{w}$这个最优解，但是需要注意，此时的$w=\tilde{w}$将不再是$J_1$的最优解，即$J_1(\tilde{w})\neq0$，因此通过最小化$J_2$求得的最优解$w=\tilde{w}$将使$J_1(\tilde{w})>J_1(\hat{w})$，而这就意味着模型$B$比模型$A$更简单了，也就代表着从一定程度上缓解了$A$的过拟合现象。

同时，由式(3-98)可知，通过增大参数$\lambda$的取值可以对应增大正则化项所对应的梯度，而这将使最后求解得到更加简单的模型（参数值更加趋近于0）。也就是$\lambda$越大，一定程度上越能缓解模型的过拟合现象，因此，参数$\lambda$又叫作惩罚项（Penalty Term）或者惩罚系数。

最后，从上面的分析可知，在第1种情况中$\mathcal{l}_2$正则化可以看作使训练好的模型不再对噪声数据那么敏感，而对于第2种情况来讲，$\mathcal{l}_2$正则化则可以看作使模型不再那么复杂，但其实两者的原理归结起来都是一回事，那就是通过较小的参数取值，使模型变得更加简单。

