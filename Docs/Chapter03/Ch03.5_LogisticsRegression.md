# 3.5 逻辑回归

在前面几节内容中笔者详细地介绍了线性回归模型的原理及其实现，在本节内容中将继续介绍下一个经典的机器学习算法——逻辑回归（Logistic Regression）。

## 3.5.1 理解逻辑回归模型

通常来讲，一个新算法的诞生要么用来改善已有的算法模型，要么就是首次提出用来解决一个新的问题，而逻辑回归模型恰恰属于后者，它是用来解决一类新的问题——分类（Classification）。什么是分类问题呢？

现在有两堆样本点，需要建立一个模型来对新输入的样本进行预测，判断其应该属于哪个类别，即二分类问题（Binary Classification），如图3-19所示。对于这个问题的描述用线性回归来解决肯定是不行的，因为两者本就属于不同类型的问题。退一步讲，即使用线性回归来建模得到的估计也就是一条向右倾斜的直线，而我们这里需要的却是一条向左倾斜的且位于两堆样本点之间的直线。同时，回归模型的预测值都位于预测曲线附近，而无法做到区分直线两边的东西。既然用已有的线性回归解决不了，那么我们可不可以在此基础上做一点改进以实现分类的目的呢？答案是当然可以。

<div align=center>
<img width="400" src="../img/p3-19.png"/>
</div>

<center>
  图 3-19 分类任务
</center>
## 3.5.2 建立逻辑回归模型

既然是解决分类问题，那么完全可以通过建立一个模型用来预测每个样本点$(x_1,x_2)$属于其中一个类别的概率$p$，如果$p＞0.5$，我们就可以认为该样本点属于这个类别，这样就能解决上述的二分类问题了。该怎样建立这个模型呢？

在前面的线性回归中，通过建模$h(x)=wx+b$来对新样本进行预测，其输出值为可能的任意实数，但此处既然要得到一个样本所属类别的概率，那最直接的办法就是通过一个函数$g(z$)，将$x_1$和$x_2$这两个特征的线性组合映射至［0,1］的范围即可。由此，便得到了逻辑回归中的预测模型
$$
\hat{y}=h(x)=g(w_1x_1+w_2x_2+b)\tag{3-1,3-45}
$$
其中，$w_1$、$w_2$和$b$为未知参数； $h(x)$称为假设函数（Hypothesis）。当$h(x)$大于某个值（通常设为0.5）时，便可以认为样本$x$属于正类，反之则认为属于负类。同时，也将$w_1x_1+w_2x_2+b=0$称为两个类别间的决策边界（Decision Boundary）。当求解得到$w_1$、$w_2$和$b$后，也就意味着得到了这个分类模型。

注意： 回归模型一般来讲是指对连续值进行预测的一类模型，而分类模型则是指对离散值（类标）预测的一类模型，但是由于历史的原因虽然逻辑回归被称为回归，但它却是一个分类模型，这算是一个例外。

当然，如果该数据集有$n$个特征维度，那么同样只需要将所有特征的线性组合映射至区间 [0,1] 即可
$$
\hat{y}=h(x)=g({{w}_{1}}{{x}_{1}}+{{w}_{2}}{{x}_{2}}+\cdots +{{w}_{n}}{{x}_{n}}+b)\tag{3-3,3-46}
$$

同时，有了前面几节关于神经网络内容的介绍，我们还可以通过如下示意图来对式(3-46)中的模型进行表示



## 3.5.3 求解逻辑回归模型

当建立好模型之后就需要找到一种方法来求解模型中的未知参数。同线性回归一样，此时也需要通过一种间接的方式，即通过目标函数来刻画预测标签（Label）与真实标签之间的差距。当最小化目标函数后，便可以得到需要求解的参数$w$和b。

对于逻辑回归来说，我们可以通过最小化式(3-47)中的目标函数来求解模型参数
$$
\begin{aligned}
  & J(w,b)=-\frac{1}{m}\left[ \sum\limits_{i=1}^{m}{{{y}^{(i)}}}\log h({{x}^{(i)}})+(1-{{y}^{(i)}})\log (1-h({{x}^{(i)}})) \right] \\[1ex] 
 & h({{x}^{(i)}})=g(w{{x}^{(i)}}+b)  
\end{aligned}\tag{3-2,3-47}
$$
其中，$m$表示样本总数，$x^{(i)}$表示第$i$个样本，$y^{(i)}$表示第$i$个样本的真实标签，取值为0或1，$h(x^{(i)})$表示第$i$个样本为正类的预测概率。

由式(3-2)可以知道，当函数$J(w,b)$取得最小值的参数$\hat{w}$和$\hat{b}$，也就是我们要求的目标参数。原因在于，当$J(w,b)$取得最小值时就意味着此时所有样本的预测标签与真实标签之间的差距最小，这同时也是最小化目标函数的意义，因此，对于如何求解模型$h(x)$的问题就转化为如何最小化目标函数$J(w,b)$的问题。

至此，对逻辑回归算法第一阶段核心内容的学习也就只差一步之遥了，也就是评价指标及通过开源的框架来建模并进行预测。

## 3.1.4 逻辑回归示例代码
