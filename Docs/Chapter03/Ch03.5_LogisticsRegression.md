# 3.5 逻辑回归

在前面几节内容中笔者详细地介绍了线性回归模型的原理及其实现，在本节内容中将继续介绍下一个经典的机器学习算法——逻辑回归（Logistic Regression）。

## 3.5.1 理解逻辑回归模型

通常来讲，一个新算法的诞生要么用来改善已有的算法模型，要么就是首次提出用来解决一个新的问题，而逻辑回归模型恰恰属于后者，它是用来解决一类新的问题——分类（Classification）。什么是分类问题呢？

现在有两堆样本点，需要建立一个模型来对新输入的样本进行预测，判断其应该属于哪个类别，即二分类问题（Binary Classification），如图3-19所示。对于这个问题的描述用线性回归来解决肯定是不行的，因为两者本就属于不同类型的问题。退一步讲，即使用线性回归来建模得到的估计也就是一条向右倾斜的直线，而我们这里需要的却是一条向左倾斜的且位于两堆样本点之间的直线。同时，回归模型的预测值都位于预测曲线附近，而无法做到区分直线两边的东西。既然用已有的线性回归解决不了，那么我们可不可以在此基础上做一点改进以实现分类的目的呢？答案是当然可以。

<div align=center>
<img width="350" src="../img/p3-19.png"/>
</div>
<center>
  图 3-19 分类任务
</center>


## 3.5.2 建立逻辑回归模型

既然是解决分类问题，那么完全可以通过建立一个模型用来预测每个样本点$(x_1,x_2)$属于其中一个类别的概率$p$，如果$p＞0.5$，我们就可以认为该样本点属于这个类别，这样就能解决上述的二分类问题了。该怎样建立这个模型呢？

在前面的线性回归中，通过建模$h(x)=wx+b$来对新样本进行预测，其输出值为可能的任意实数，但此处既然要得到一个样本所属类别的概率，那最直接的办法就是通过一个函数$g(z$)，将$x_1$和$x_2$这两个特征的线性组合映射至［0,1］的范围即可。由此，便得到了逻辑回归中的预测模型
$$
\hat{y}=h(x)=g(w_1x_1+w_2x_2+b)\tag{3-1,3-45}
$$
其中，$w_1$、$w_2$和$b$为未知参数； $h(x)$称为假设函数（Hypothesis）。当$h(x)$大于某个值（通常设为0.5）时，便可以认为样本$x$属于正类，反之则认为属于负类。同时，也将$w_1x_1+w_2x_2+b=0$称为两个类别间的决策边界（Decision Boundary）。当求解得到$w_1$、$w_2$和$b$后，也就意味着得到了这个分类模型。

注意： 回归模型一般来讲是指对连续值进行预测的一类模型，而分类模型则是指对离散值（类标）预测的一类模型，但是由于历史的原因虽然逻辑回归被称为回归，但它却是一个分类模型，这算是一个例外。

当然，如果该数据集有$n$个特征维度，那么同样只需要将所有特征的线性组合映射至区间 [0,1] 即可
$$
\hat{y}=h(x)=g({{w}_{1}}{{x}_{1}}+{{w}_{2}}{{x}_{2}}+\cdots +{{w}_{n}}{{x}_{n}}+b)\tag{3-3,3-46}
$$

同时，有了前面几节关于神经网络内容的介绍，我们还可以通过如下示意图来对式(3-46)中的模型进行表示



<font color = red>画一个图，n个神经元，一个输出</font>

<center>
  图 3-20 逻辑回归模型结构图（偏置未画出）
</center>


## 3.5.3 求解逻辑回归模型

当建立好模型之后就需要找到一种方法来求解模型中的未知参数。同线性回归一样，此时也需要通过一种间接的方式，即通过目标函数来刻画预测标签（Label）与真实标签之间的差距。当最小化目标函数后，便可以得到需要求解的参数$w$和b。

对于逻辑回归来说，我们可以通过最小化式(3-47)中的目标函数来求解模型参数
$$
\begin{aligned}
  & J(w,b)=-\frac{1}{m}\left[ \sum\limits_{i=1}^{m}{{{y}^{(i)}}}\log h({{x}^{(i)}})+(1-{{y}^{(i)}})\log (1-h({{x}^{(i)}})) \right] \\[1ex] 
 & h({{x}^{(i)}})=g(w{{x}^{(i)}}+b)  
\end{aligned}\tag{3-2,3-47}
$$
其中，$m$表示样本总数，$x^{(i)}$表示第$i$个样本，$y^{(i)}$表示第$i$个样本的真实标签，取值为0或1，$h(x^{(i)})$表示第$i$个样本为正类的预测概率。

由式(3-47)可知，当函数$J(w,b)$取得最小值的参数$\hat{w}$和$\hat{b}$，也就是我们要求的目标参数。原因在于，当$J(w,b)$取得最小值时就意味着此时所有样本的预测标签与真实标签之间的差距最小，这同时也是最小化目标函数的意义，因此，对于如何求解模型$h(x)$的问题就转化为如何最小化目标函数$J(w,b)$的问题。

## 3.5.4 从二分类到多分类

在说完逻辑回归这个二分类模型后自然而然我们就会想到如何进行多分类的任务，因为在实际情况中，绝大多数任务场景都不会是一个简单的二分类任务。通常情况下在用逻辑回归处理多分类任务时，都会采样一种称为One-vs-all（也叫作 One-vs-rest）的方法。这种策略的核心思想是每次将其中一个类别的样本和剩余的其他类的样本两者看作一个二分类任务进行模型训练，最后在预测过程中选择输出概率值最大那个模型对应的类别作为该样本点的所属类别。

例如，对于某个包含有4个特征维度及3个类别的数据集来说，便可以建立3个二分类模型$h_0(x)$、$h_1(x)$和$h_2(x)$来完成整个3分类任务。

<img src="https://moonhotel.oss-cn-shanghai.aliyuncs.com/images/000132.png" style="zoom:60%;" />

<center>
    图 3-21. 三分类模型结构示意图（偏置未画出）
</center>

如图3-21所示便是3个逻辑回归的结构图，并且在训练模型时需要对每个样本的类标重新进行编码。例如有5个样本的原始标签为`[0,0,1,2,1]`，那么在训练$h_0(x)$这个模型时这5个标签将会变为`[1,1,0,0,0]`。同理，在训练$h_1(x)$和$h_2(x)$时，样本标签将会重新编码为`[0,0,1,0,1]`和`[0,0,0,1,0]`。最后，对于每个新样本来说，其预测结果为$h_0(x)$、$h_1(x)$和$h_2(x)$这3个模型中概率值最大的模型对应的类标。

当然，对于图3-21所示的这种表示方法来说，当分类类别较多时表示起来就不那么简洁。由于图3-21中每个模型的输入均相同，因此可以简化为如图3-22所示的形式。

<img src="https://moonhotel.oss-cn-shanghai.aliyuncs.com/images/000133.png" style="zoom:70%;" />

<center>
    图 3-22. 三分类模型结构示意图（偏置未画出）
</center>

从图3-22可以看出，此时图3-21里所示的3个模型已经被简化到了一个结构中，并且除了简化整个模型结构之外，图3-22所示的3个模型还能一次同时进行训练并输出3个结果。因此，在这样的条件下，模型训练时的样本标签将会被重新编码为另外一种形式。仍旧以上面5个样本的标签为例，第1个样本的标签将被编码为`[1,0,0]`、第2个样本的标签将被编码为`[1,0,0]`，后续3个依次为`[0,1,0]`、`[0,0,1]`和`[0,1,0]`，以此来分别与图3-22中模型的3个输出进行损失计算。同时，这种形式的编码在深度学习中被称为独热（One-hot）编码。

## 3.5.5 特征的意义

在第3.1节内容中，笔者从线性回归里的房价预测到梯形块面积介绍了输入特征对于模型预测结果的重要性，接着又从特征提取以及非线性变换的角度介绍了特征提取对于模型的重要性，最后从单层神经网络（线性回归模型）顺利的过渡到了多层神经网络，也就是深度学习的概念中，当然这样的理念也同样体现在分类模型中。

与传统的机器学习相比，深度学习最大的不同点便在于特征的可解释性。在机器学习中，我们会尽可能的要求每个特征（包括不同特征之间组合得到的新特征）都具有一定的含义。例如在第3.1.4节中介绍的梯形面积预测示例中，每个特征$x_1$和$x_2$以及手工构造出来的新特征$x_1x_2$、$x_1^2$和$x^2_2$都具有极强的可解释性。因此，在机器学习中基本上不存在所谓“抽象特征”的概念。但是，当我们用机器学习算法来做某一些分类任务时却又不得不用这些不知道什么意思的特征。

例如通过图3-22所示的模型对图3-23中的手写体数字进行分类时，通常做法就是将整个图片展开成一列64维的向量（像素值）输入到模型中进行分类。

<img src="https://moonhotel.oss-cn-shanghai.aliyuncs.com/images/p3-23.jpg" alt="p3-23" style="zoom:30%;" />

<center>
  图 3-22. 深层特征提取示意图
</center>

例如对于图3-22所示的数字8来说，其展开后的向量表示为：

```python
1 [ 0.  0. 10. 14.  8.  1.  0.  0.  0.  2. 16. 14.  6.  1.  0.  0.  0.  0.  15. 15.  8. 15.  0.  0.  0.  0.  5. 16. 16. 10.  0.  0.  0.  0. 12. 15.  15. 12.  0.  0.  0.  4. 16.  6.  4. 16.  6.  0.  0.  8. 16. 10.  8. 16.  8.  0.  0.  1.  8. 12. 14. 12.  1.  0.]
```

其含义为，图3-22中第1行第1~2个格子为0即为白色，接着第3个格子为10即灰色，且越接近16的各自颜色月偏向于黑色，其它以此类推。

如果现在让你说出上述64个特征每个特征维度的含义你能说清楚吗？显然是不能的，不过这依然不影响模型最后的分类结果，但这又是为什么呢？

有件事，叫天不知地知，你不知我知，那就是我的鞋底有个洞。对于即使给模型输入在我们人类眼里不知道是什么含义的特征，模型依旧能够展现出很好的分类效果这件事情，可能也就是模型知而恰恰我们不知。但这样解释合理吗？笔者认为这是合理的。想一想，狗主要是靠什么来辨识事物的？对，就是味道。但我们人能通过“味道”这个特征来辨识事物吗？类似的还有蝙蝠能够通过声波这个特征来辨识事物等，而这些都是我们人所不能的。那既然是这样，为什么不可以认为是模型具备了某种我们人所不能的特征识别能力呢？
