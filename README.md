# DeepLearningWithMe
# 《跟我一起深度学习》

## 环境配置

`Python`版本为3.6，其它包的版本信息见`requirements.txt`，使用如下命令即可完成安装。

```python
pip install -r requirements.txt
```

## 第一部分：前馈神经网络 [代码](./01_DeepForwardNN/README.md)

- [你告诉我什么是深度学习](https://mp.weixin.qq.com/s/AEBhMs2Ke2XR2GbJFUAuYQ)
- [这样拟合正弦函数你会吗？](https://mp.weixin.qq.com/s/wQecLuwS9oqEcaB0d9ksSA)
- [Pytorch之Linear与MSELoss](https://mp.weixin.qq.com/s/48OAbo7mR3FSd16lg03Q-g)
- [想明白多分类必须得谈逻辑回归](https://mp.weixin.qq.com/s/sC_W3oCX7zgpjafGucZj-A)
- [Pytorch之Softmax多分类任务](https://mp.weixin.qq.com/s/b_4_wiJ8E1NPeGQw-cS-ww)
- [Pytorch之简洁实现Softmax多分类](https://mp.weixin.qq.com/s/qd-1ya3IjJiqd8rxZSG7FQ)
- [我告诉你什么是深度学习](https://mp.weixin.qq.com/s/0U1iNl3cl-rzoVogZD_WcA)
- [Pytorch之多层感知机分类任务](https://mp.weixin.qq.com/s/uYC47N8Vp6xW8hWXkBgKpw)
- [`l.backward`你到底是个什么东西？](https://mp.weixin.qq.com/s/9CBGWrMsq5GGr-nlz2ie4w)
- [你还不会实现反向传播?](https://mp.weixin.qq.com/s/KdlOuuzfwqbNQNUMZXp5Mw)
- [Softmax+Cross entropy 梯度检验](https://mp.weixin.qq.com/s/uTVv8EJPSdEiApZk11ubSw)

## 第二部分：卷积神经网络 [代码](./02_ConvolutionalNN/README.md)

- [看不懂卷积或许只是因为](https://mp.weixin.qq.com/s/Bu6JDoXfiiNklw8UGmjxpA)
- [原来卷积是这么计算的](https://zhuanlan.zhihu.com/p/268179286)
- [卷积操作中的填充与池化](https://mp.weixin.qq.com/s/3xYDFe2gcZcc9L24U3cvgw)
- [卷积池化与LeNet5网络模型](https://mp.weixin.qq.com/s/bFO1eaCCMERyvu4QSGyeaQ)
- [LeNet5的继任者AlexNet模型](https://mp.weixin.qq.com/s/ckm_P7T219k8UGUVsI88OA)
- [VGG一个可使用重复元素的网络](https://mp.weixin.qq.com/s/X7VDKcWTRdPvbOZ1Oz7TkQ)
- [NiN一个即使放到现在也不会过时的网络](https://mp.weixin.qq.com/s/dA1AATIrFMTjMk8FYGPqPw)
- [厉害了！能把多尺度卷积说得这么高大上](https://mp.weixin.qq.com/s/3Z-_f4p73V20HEpEHsendA)
- [你看这个网络它又宽又长](https://mp.weixin.qq.com/s/OWINukstH87Yldl99_gItw)
- [Top-K准确率介绍与实现](https://mp.weixin.qq.com/s/b7ZxUlbyC_aYZ2Fs7ReK2Q)
- [不得不说的Batch Normalization](https://mp.weixin.qq.com/s/rqnKx-3F6YycfOVfN5JPlQ)
- [Batch Normalization分析与实现](https://mp.weixin.qq.com/s/0vH7E2zw0vzr8J1bcNLbkA)
- [多标签分类中的损失函数与评价指标](https://mp.weixin.qq.com/s/85a0I3JsXUCAPrLEvkePrg)


## 第三部分：循环神经网络 [代码](03_RcurrentNN/README.md)
- [你还在手动构造词表？试试`torchtext.vocab`](https://www.ylkz.life/deeplearning/p10449077/)
- [快速从零构建成DataLoader](https://www.ylkz.life/deeplearning/p10375540/)


## 第四部分：模型训练 [代码](04_ModelTraining/README.md)
- [PyTorch中模型的保存与迁移](https://www.ylkz.life/deeplearning/p12977315/)
- [训练模型时如何便捷保存日志？](https://www.ylkz.life/tools/p10958151/)
- [Transformers自定义学习了动态调整](https://www.ylkz.life/deeplearning/p10462014/)

## 第五部分： Transformer网络模型 [代码](05_Transformer/README.md)
- [This post is all you need（①多头注意力机制原理）](https://www.ylkz.life/deeplearning/p10553832/) 
- [This post is all you need（②位置编码与编码解码过程）](https://www.ylkz.life/deeplearning/p10770524/)
- [This post is all you need（③网络结构与自注意力实现）](https://www.ylkz.life/deeplearning/p12158901/)
- [This post is all you need（④Transformer的实现过程）](https://www.ylkz.life/deeplearning/p10391698/)
- [This post is all you need（⑤基于Transformer的翻译模型）](https://www.ylkz.life/deeplearning/p10667939/)
- [This post is all you need（⑥基于Transformer的分类模型）](https://www.ylkz.life/deeplearning/p10550146/)
- [This post is all you need（⑦基于Transformer的对联模型）](https://www.ylkz.life/deeplearning/p11017569/)
- [This post is all you need（上卷）——层层剥开Transformer](https://mp.weixin.qq.com/s/uch_AGcSB8OSAeVu2sme8A)
- [1. BERT原理与NSL和MLM](https://www.ylkz.life/deeplearning/p10631450/)
- [2. 从零实现BERT网络模型](https://www.ylkz.life/deeplearning/p10602241/) 
- [3. 基于BERT预训练模型的中文文本分类任务](https://www.ylkz.life/deeplearning/p10979382/)
- [4. 基于BERT预训练模型的英文文本蕴含(MNLI)任务](https://www.ylkz.life/deeplearning/p10407402/)
- [5. 基于BERT预训练模型的英文多选项(SWAG)任务](https://mp.weixin.qq.com/s/GqsbMBNt9XcFIjmumR04Pg)
- [6. 基于BERT预训练模型的英文问答(SQuAD)任务](https://www.ylkz.life/deeplearning/p10265968/) 
- [7. 基于NSL和MLM任务从头训练BERT任务](https://mp.weixin.qq.com/s/2Vtxv1Wj9knEFKUyUeQ_6w)
- [This post is all you need（下卷）——步步走进BERT](https://mp.weixin.qq.com/s/tQELF16n5O1-3cvapeaf4Q)